services:
  postgres:
    image: postgres:latest
    container_name: psql_service
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    networks:
      - weather_network
    volumes:
      - psql_data:/var/lib/postgresql
      - ./src/utils/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 5
    command: >
      sh -c "
        echo \"listen_addresses = '*'\" >> /var/lib/postgresql/postgresql.conf &&
        echo \"host all all 0.0.0.0/0 md5\" >> /var/lib/postgresql/pg_hba.conf &&
        docker-entrypoint.sh postgres
      "

  extract:
    build: .
    container_name: extract_script
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - API_KEY=${API_KEY}
      - DB_HOST=postgres
      - DB_NAME=${DB_NAME}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_PORT=5432
    networks:
      - weather_network
    command: >
      sh -c "
      ./scripts/wait-for-postgres.sh postgres postgres &&
      python /app/src/extract/extract.py"
  
  namenode:
    image: bde2020/hadoop-namenode:latest
    container_name: namenode
    restart: always
    ports:
      - 9870:9870  # web_ui
      - 9000:9000  # hdfs
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=${CLUSTER_NAME}
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
    #   # spark-network:
      weather_network:
 

  datanode1:
    image: bde2020/hadoop-datanode:latest
    container_name: datanode1
    restart: always
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
    #   # spark-network:
      weather_network:

  datanode2:
    image: bde2020/hadoop-datanode:latest
    container_name: datanode2
    restart: always
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      # spark-network:
      weather_network:

  datanode3:
    image: bde2020/hadoop-datanode:latest
    container_name: datanode3
    restart: always
    volumes:
      - hadoop_datanode3:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
    #   # spark-network:
      weather_network:

networks:
  weather_network:
    driver: bridge
  # spark-network:
  #   driver: bridge

volumes:
  psql_data:
    driver: local
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3: